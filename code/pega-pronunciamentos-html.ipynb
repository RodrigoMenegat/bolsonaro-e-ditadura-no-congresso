{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pega-pronunciamentos-html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Esse script usa os csvs gerados em `pega-links` e extrai os pronunciamentos para os quais existem links para conteúdo em html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO DO:\n",
    "- Implementar função `download_pdf()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importa pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import pprint as pp\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Chama tqdm para monitorar progresso no pandas\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lê dados externos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(name, database):\n",
    "    \n",
    "    '''\n",
    "    Lê arquivos .csv usando o pandas. Os arquivos devem\n",
    "    seguir o padrão de nome '{nome-parlamentar}-{database}-metadada.csv'\n",
    "    \n",
    "    Parâmetros:\n",
    "    \n",
    "    name -> Nome do parlamentar separado por hífen (como \"jair-bolsonaro\")\n",
    "    database -> \"plenario\" ou \"comissao\"\n",
    "    '''\n",
    "    \n",
    "    fpath = \"../data/tables/{name}-{database}-metadata.csv\".format(\n",
    "        name = name, \n",
    "        database = database\n",
    "    )\n",
    "    \n",
    "    df = pd.read_csv(fpath, dtype='str')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funções para extrair dados de um link e salvá-lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_request(url):\n",
    "    \n",
    "    '''\n",
    "    Recebe uma url, faz requisição get\n",
    "    e retorna uma string de texto.\n",
    "    '''\n",
    "\n",
    "    headers = {\"User-Agent\":\"Rodrigo Menegat, jornalista (rodrigoschuinski@gmail.com)\"}\n",
    "    r = requests.get(url, headers = headers)\n",
    "    r.encoding = 'utf-8'\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_soup(data):\n",
    "    \n",
    "    '''\n",
    "    Recebe uma string de texto e retorna um objeto do BeautifulSoup.\n",
    "    '''\n",
    "    \n",
    "    # Preciso retirar as tags </b> do texto porque\n",
    "    # o desenvolvedor da Câmara colocou elas fora\n",
    "    # de ordem e isso faz o raspador enlouquecer.\n",
    "    #                 ¯\\_(ツ)_/¯\n",
    "    data = data.replace(\"<b>\",\"\").replace(\"</b>\",\"\")\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_content_plen(soup):\n",
    "    \n",
    "    '''\n",
    "    Usa o BeautifulSoup para extrair o conteúdo da URL\n",
    "    em que está o discurso. Retorna uma string de texto.\n",
    "    '''\n",
    "        \n",
    "    paragraph = soup.find( \"p\", attrs = {\"align\":\"justify\"} )\n",
    "    \n",
    "    # Esse passo a mais é necessário porque, em alguns casos, há mais de uma tag 'font' dentro do 'p'.\n",
    "    # Nessas ocasiões, não era retornado todo o texto, mas apenas o conteúdo da primeira tag\n",
    "    fonts = paragraph.find_all(\"font\")\n",
    "    \n",
    "    text = [ font.text.strip() for font in fonts ]\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_content_com(soup):\n",
    "    \n",
    "    '''\n",
    "    Usa o BeautifulSoup para extrair o conteúdo da URL\n",
    "    em que está o discurso. Retorna uma string de texto.\n",
    "    '''\n",
    "    \n",
    "    fonts = soup.find_all(\"font\")\n",
    "    fonts = [ font.text.strip() for font in fonts ]\n",
    "    content = ' '.join(fonts)\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_txt(content, path, *args):\n",
    "    \n",
    "    '''\n",
    "    Salva um arquivo txt no path designado.\n",
    "    O array args é usado para gerar o nome do arquivo.\n",
    "    '''\n",
    "   \n",
    "    file_id = '-'.join(args)\n",
    "    file_id = file_id.replace(\"/\",\"\").replace(\".\",\"-\")\n",
    "    fname = path + file_id + \".txt\"\n",
    "    \n",
    "    with open(fname, 'w+') as file:\n",
    "        file.write(content)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_pdf(url, date, path):\n",
    "    \n",
    "    '''\n",
    "    Função para baixar o pdf do documento\n",
    "    onde está registrado um pronunciamento.\n",
    "    '''\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    \n",
    "    # TO DO - implementar captura de sessões conjuntas,\n",
    "    # cujo link leva para um PDF viewer feioso\n",
    "    \n",
    "    # Pega redirecionamentos\n",
    "    if r.history:    \n",
    "        # Acessa a tag de redirecionamento e pega o contúdo\n",
    "        r = requests.get(url)\n",
    "        soup = make_soup(r.text)\n",
    "        \n",
    "       # Acha o link de redirecionamento\n",
    "        meta = soup.find('meta',attrs={'http-equiv':'Refresh'})\n",
    "        meta = str(meta)\n",
    "        meta = re.search(\".*URL=(.*)\\\" http.*\", meta)\n",
    "        \n",
    "        # Se encontrado, refaz a solicitação\n",
    "        if meta:\n",
    "            url = meta.group(1)\n",
    "            r =  requests.get(url)\n",
    "            \n",
    "            # Seleciona página da declaração para salvar no nome do arquivo\n",
    "            page_no = re.search(\"page=(\\d+)\", url).group(1)\n",
    "            fpath = \"{path}{date}-pg-{page_no}.pdf\"\n",
    "            fpath = fpath.format(path = path, \n",
    "                                 date = date, \n",
    "                                 page_no = page_no)\n",
    "    \n",
    "            if not os.path.isfile(fpath):\n",
    "                with open(fpath, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "    \n",
    "    else:\n",
    "        print(\"Couldn't download the following url:\\n\" + url)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função que, via df.apply, as funções acima em cada linha do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_row_plen(row):\n",
    "    \n",
    "    '''\n",
    "    Função para executar via df.apply()\n",
    "    que acessa os dados de cada linha do\n",
    "    dataframe e executa as operações ne-\n",
    "    cessárias para acessar os dados. Usar\n",
    "    apenas com a tabela com discursos feitos\n",
    "    no plenário.\n",
    "    '''\n",
    "    \n",
    "    if row.Discurso != '-':\n",
    "    \n",
    "        url = row.Discurso\n",
    "        data = make_request(url)\n",
    "        soup = make_soup(data)\n",
    "        txt = scrape_content_plen(soup)\n",
    "        \n",
    "        path = \"../data/txts/plenario/\"\n",
    "        file_id = [ str(row.name), row.Data, row.Hora, row.Fase, row.Sessão ]\n",
    "        save_txt(txt, path, *file_id)\n",
    "    \n",
    "        # time.sleep(.2)\n",
    "    \n",
    "    else:\n",
    "        url = row.Publicação\n",
    "        path = \"../data/pdfs/plenario/\"\n",
    "        date = row.Data.replace('/','-')\n",
    "        # download_pdf(url, date, path)\n",
    "\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_row_com(row):\n",
    "\n",
    "    '''\n",
    "    Função para executar via df.apply()\n",
    "    que acessa os dados de cada linha do\n",
    "    dataframe e executa as operações ne-\n",
    "    cessárias para acessar os dados. Usar\n",
    "    apenas com a tabela com discursos feitos\n",
    "    no plenário.\n",
    "    '''\n",
    "    \n",
    "    url = row.Texto\n",
    "    data = make_request(url)\n",
    "    soup = make_soup(data)\n",
    "    txt = scrape_content_com(soup)\n",
    "\n",
    "    path = \"../data/txts/comissao/\"\n",
    "    file_id = [row.Data, row.Hora, row.Reunião]\n",
    "    save_txt(txt, path, *file_id)\n",
    "\n",
    "    # time.sleep(.2)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roda funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_df(df):\n",
    "    \n",
    "    '''\n",
    "    Função que encapsula os métodos de raspagem aplicados via df.apply().\n",
    "    Ela é útil para detectar com qual tabela (plenário ou comissões)\n",
    "    estamos lidando e fazer a operação correta, sem precisar se preocupar\n",
    "    com selecionar a função específica para o dataframe.\n",
    "    '''\n",
    "\n",
    "    if 'Sessão' in df.columns:\n",
    "        df.progress_apply(scrape_row_plen, axis = 1)\n",
    "    elif 'Comissão' in df.columns:\n",
    "        df.progress_apply(scrape_row_com, axis = 1)\n",
    "    else:\n",
    "        raise Exception(\"O dataframe não está em um formato conhecido.\")\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_plen = read_data('jair-bolsonaro', 'plenario')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd3ff1844b44b01b32135b8a71ecb48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scrape_df(df_plen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_com = read_data('jair-bolsonaro', 'comissao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scrape_df(df_com)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
